"""
MMWHSæ•°æ®é›†å®Œæ•´å¤„ç†æµç¨‹ï¼ˆä¿®å¤ç‰ˆï¼‰
åŒ…æ‹¬ï¼šæ•°æ®é¢„å¤„ç†ã€è´¨é‡æ£€æµ‹ã€é…å¯¹éªŒè¯
å…³é”®ä¿®å¤ï¼š
1. ç»Ÿè®¡ä¿¡æ¯ä¿å­˜ç§»åˆ°å¾ªç¯å¤–
2. é™ä½è´¨é‡ç­›é€‰é˜ˆå€¼
3. æ”¯æŒæµ‹è¯•é›†æ— æ ‡æ³¨
"""

import os
import numpy as np
import nibabel as nib
from pathlib import Path
import matplotlib.pyplot as plt
from tqdm import tqdm
from PIL import Image
import json


class MMWHSPreprocessor:
    """MMWHSæ•°æ®é›†é¢„å¤„ç†å™¨"""

    def __init__(self, data_root, output_root):
        self.data_root = Path(data_root)
        self.output_root = Path(output_root)

        # æ•°æ®è·¯å¾„
        self.ct_train_dir = self.data_root / 'ct_train'
        self.ct_test_dir = self.data_root / 'ct_test'
        self.mr_train_dir = self.data_root / 'mr_train'
        self.mr_test_dir = self.data_root / 'mr_test'

        # è¾“å‡ºè·¯å¾„
        self.output_root.mkdir(parents=True, exist_ok=True)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'train': {
                'ct': {'volumes': [], 'slices': [], 'quality': [], 'spacing': [], 'dimensions': []},
                'mr': {'volumes': [], 'slices': [], 'quality': [], 'spacing': [], 'dimensions': []}
            },
            'test': {
                'ct': {'volumes': [], 'slices': [], 'quality': [], 'spacing': [], 'dimensions': []},
                'mr': {'volumes': [], 'slices': [], 'quality': [], 'spacing': [], 'dimensions': []}
            }
        }

    def load_nifti(self, file_path):
        """åŠ è½½NIfTIæ–‡ä»¶"""
        try:
            img = nib.load(str(file_path))
            data = img.get_fdata()
            affine = img.affine
            header = img.header
            spacing = header.get_zooms()
            return data, affine, header, spacing
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {file_path}")
            print(f"   é”™è¯¯: {e}")
            return None, None, None, None

    def normalize_intensity(self, img, modality='ct'):
        """æ ‡å‡†åŒ–å›¾åƒå¼ºåº¦"""
        if modality == 'ct':
            # CT: å¿ƒè„çª—
            window_center = 40
            window_width = 400
            img_min = window_center - window_width // 2
            img_max = window_center + window_width // 2

            img = np.clip(img, img_min, img_max)
            img = (img - img_min) / (img_max - img_min) * 255

        else:  # MR
            # ç™¾åˆ†ä½æ•°å½’ä¸€åŒ–
            valid_pixels = img[img > 0]
            if len(valid_pixels) > 0:
                p1 = np.percentile(valid_pixels, 1)
                p99 = np.percentile(valid_pixels, 99)
                img = np.clip(img, p1, p99)
                img = (img - p1) / (p99 - p1 + 1e-8) * 255
            else:
                img = np.zeros_like(img)

        return img.astype(np.uint8)

    def check_slice_quality(self, slice_img, mask_slice=None):
        """
        æ£€æŸ¥åˆ‡ç‰‡è´¨é‡ï¼ˆå®½æ¾ç‰ˆæœ¬ï¼‰
        """
        quality_score = 1.0
        issues = []

        # 1. æ£€æŸ¥æ˜¯å¦å…¨é»‘
        if np.max(slice_img) == 0:
            return 0.0, ['å…¨é»‘åˆ‡ç‰‡']

        # 2. æ£€æŸ¥å¯¹æ¯”åº¦ï¼ˆé™ä½æƒ©ç½šï¼‰
        contrast = np.std(slice_img)
        if contrast < 5:  # â† ä»10é™åˆ°5
            quality_score *= 0.7  # â† ä»0.5æ”¹åˆ°0.7
            issues.append(f'ä½å¯¹æ¯”åº¦({contrast:.1f})')

        # 3. æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡ï¼ˆé™ä½æƒ©ç½šï¼‰
        if mask_slice is not None:
            target_pixels = np.sum(mask_slice > 0)
            if target_pixels == 0:
                quality_score *= 0.5  # â† ä»0.3æ”¹åˆ°0.5
                issues.append('æ— ç›®æ ‡')
            elif target_pixels < 50:  # â† ä»100é™åˆ°50
                quality_score *= 0.8  # â† ä»0.7æ”¹åˆ°0.8
                issues.append(f'ç›®æ ‡å°({target_pixels}px)')

        # 4. ä¿¡å™ªæ¯”æ£€æŸ¥ï¼ˆé™ä½æƒ©ç½šï¼‰
        foreground = slice_img[slice_img > np.percentile(slice_img, 10)]
        if len(foreground) > 0:
            signal = np.mean(foreground)
            noise = np.std(foreground)
            if noise > 0:
                snr = signal / noise
                if snr < 1.5:  # â† ä»2é™åˆ°1.5
                    quality_score *= 0.8  # â† ä»0.7æ”¹åˆ°0.8
                    issues.append(f'ä½SNR({snr:.1f})')

        return min(quality_score, 1.0), issues

    def process_volume(self, img_path, mask_path, output_dir, modality, volume_id):
        """
        å¤„ç†å•ä¸ª3D volumeï¼ˆä¿®å¤ç‰ˆï¼‰
        âœ… å…³é”®ä¿®å¤ï¼šç»Ÿè®¡ä¿¡æ¯ä¿å­˜ç§»åˆ°å¾ªç¯å¤–
        """
        print(f"\nå¤„ç† {modality.upper()} Volume {volume_id}: {img_path.name}")

        # åŠ è½½å›¾åƒ
        img_data, img_affine, img_header, img_spacing = self.load_nifti(img_path)
        if img_data is None:
            return 0

        # åŠ è½½æ ‡æ³¨
        mask_data = None
        if mask_path and mask_path.exists():
            mask_data, _, _, _ = self.load_nifti(mask_path)
            if mask_data is not None:
                unique_labels = np.unique(mask_data)
                print(f"  âœ… æ ‡æ³¨ç±»åˆ«: {unique_labels}")
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼ˆæµ‹è¯•é›†ï¼‰")

        print(f"  å›¾åƒå½¢çŠ¶: {img_data.shape}")
        print(f"  ä½“ç´ é—´è·: {img_spacing}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        img_output_dir = output_dir / modality / 'images'
        mask_output_dir = output_dir / modality / 'masks'
        img_output_dir.mkdir(parents=True, exist_ok=True)
        mask_output_dir.mkdir(parents=True, exist_ok=True)

        # é€åˆ‡ç‰‡å¤„ç†
        num_slices = img_data.shape[2]
        valid_slices = 0
        quality_scores = []

        for slice_idx in tqdm(range(num_slices), desc=f"  å¤„ç†åˆ‡ç‰‡", leave=False):
            # æå–åˆ‡ç‰‡
            img_slice = img_data[:, :, slice_idx]
            mask_slice = mask_data[:, :, slice_idx] if mask_data is not None else None

            # æ£€æŸ¥è´¨é‡
            quality, issues = self.check_slice_quality(img_slice, mask_slice)
            quality_scores.append(quality)

            # âœ… é™ä½è´¨é‡é˜ˆå€¼åˆ°0.2ï¼ˆè€Œä¸æ˜¯0.3ï¼‰
            if quality < 0.2:
                continue

            # æ ‡å‡†åŒ–
            img_slice_norm = self.normalize_intensity(img_slice, modality)

            # è°ƒæ•´å¤§å°
            img_pil = Image.fromarray(img_slice_norm)
            img_resized = img_pil.resize((256, 256), Image.BILINEAR)

            # ä¿å­˜å›¾åƒ
            save_name = f"{modality}_vol{volume_id:03d}_slice{slice_idx:03d}.png"
            img_resized.save(img_output_dir / save_name)

            # ä¿å­˜æ ‡æ³¨
            if mask_slice is not None:
                mask_binary = (mask_slice > 0).astype(np.uint8) * 255
                mask_pil = Image.fromarray(mask_binary)
                mask_resized = mask_pil.resize((256, 256), Image.NEAREST)
                mask_resized.save(mask_output_dir / save_name)
            # æµ‹è¯•é›†æ²¡æœ‰æ ‡æ³¨ï¼Œä¸ä¿å­˜mask

            valid_slices += 1

        # âœ…âœ…âœ… å…³é”®ä¿®å¤ï¼šç»Ÿè®¡ä¿¡æ¯ä¿å­˜è¦åœ¨å¾ªç¯å¤–é¢ï¼
        avg_quality = np.mean(quality_scores) if quality_scores else 0.0
        split = 'test' if 'test' in str(output_dir) else 'train'

        self.stats[split][modality]['volumes'].append(volume_id)
        self.stats[split][modality]['slices'].append(valid_slices)
        self.stats[split][modality]['quality'].append(avg_quality)
        self.stats[split][modality]['spacing'].append(img_spacing)
        self.stats[split][modality]['dimensions'].append(img_data.shape)

        print(f"  âœ… æœ‰æ•ˆåˆ‡ç‰‡: {valid_slices}/{num_slices} (è´¨é‡åˆ†æ•°: {avg_quality:.3f})")

        return valid_slices

    def process_split(self, split='train'):
        """
        å¤„ç†æŒ‡å®šçš„æ•°æ®é›†åˆ†å‰²
        âœ… æ”¯æŒæµ‹è¯•é›†æ— æ ‡æ³¨
        """

        if split == 'train':
            ct_dir = self.ct_train_dir
            mr_dir = self.mr_train_dir
        else:
            ct_dir = self.ct_test_dir
            mr_dir = self.mr_test_dir

        print(f"\nğŸ“‚ æ•°æ®ç›®å½•ï¼š")
        print(f"  CT: {ct_dir} (å­˜åœ¨: {ct_dir.exists()})")
        print(f"  MR: {mr_dir} (å­˜åœ¨: {mr_dir.exists()})")

        ct_files = {'images': [], 'labels': []}
        mr_files = {'images': [], 'labels': []}

        # æ‰«æCT
        if ct_dir.exists():
            print(f"\nğŸ” æ‰«æCT {split}é›†...")
            all_ct_files = list(ct_dir.glob('*.nii*'))
            print(f"  CTç›®å½•ä¸‹å…±æœ‰ {len(all_ct_files)} ä¸ª.niiæ–‡ä»¶")

            for file in all_ct_files:
                if 'image' in file.name:
                    ct_files['images'].append(file)
                elif 'label' in file.name:
                    ct_files['labels'].append(file)

            ct_files['images'] = sorted(ct_files['images'])
            ct_files['labels'] = sorted(ct_files['labels'])

            print(f"  âœ… CTå›¾åƒ: {len(ct_files['images'])} ä¸ª")
            print(f"  âœ… CTæ ‡æ³¨: {len(ct_files['labels'])} ä¸ª")

            if len(ct_files['images']) > 0:
                print(f"  ç¤ºä¾‹æ–‡ä»¶: {ct_files['images'][0].name}")

        # æ‰«æMR
        if mr_dir.exists():
            print(f"\nğŸ” æ‰«æMR {split}é›†...")
            all_mr_files = list(mr_dir.glob('*.nii*'))
            print(f"  MRç›®å½•ä¸‹å…±æœ‰ {len(all_mr_files)} ä¸ª.niiæ–‡ä»¶")

            for file in all_mr_files:
                if 'image' in file.name:
                    mr_files['images'].append(file)
                elif 'label' in file.name:
                    mr_files['labels'].append(file)

            mr_files['images'] = sorted(mr_files['images'])
            mr_files['labels'] = sorted(mr_files['labels'])

            print(f"  âœ… MRå›¾åƒ: {len(mr_files['images'])} ä¸ª")
            print(f"  âœ… MRæ ‡æ³¨: {len(mr_files['labels'])} ä¸ª")

            if len(mr_files['images']) > 0:
                print(f"  ç¤ºä¾‹æ–‡ä»¶: {mr_files['images'][0].name}")

        # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œè¿”å›
        if len(ct_files['images']) == 0 and len(mr_files['images']) == 0:
            print(f"\nâš ï¸  {split}é›†ä¸­æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡...")
            return

        # å¤„ç†CT
        if len(ct_files['images']) > 0:
            print(f"\n{'=' * 80}")
            print(f"ã€å¤„ç†CT {split.upper()}é›†ã€‘")
            print(f"{'=' * 80}")

            for idx, img_path in enumerate(ct_files['images'], 1):
                label_name = img_path.name.replace('_image', '_label')
                mask_path = img_path.parent / label_name
                if not mask_path.exists():
                    mask_path = None

                self.process_volume(
                    img_path=img_path,
                    mask_path=mask_path,
                    output_dir=self.output_root / split,
                    modality='ct',
                    volume_id=idx
                )

        # å¤„ç†MR
        if len(mr_files['images']) > 0:
            print(f"\n{'=' * 80}")
            print(f"ã€å¤„ç†MR {split.upper()}é›†ã€‘")
            print(f"{'=' * 80}")

            for idx, img_path in enumerate(mr_files['images'], 1):
                label_name = img_path.name.replace('_image', '_label')
                mask_path = img_path.parent / label_name
                if not mask_path.exists():
                    mask_path = None

                self.process_volume(
                    img_path=img_path,
                    mask_path=mask_path,
                    output_dir=self.output_root / split,
                    modality='mr',
                    volume_id=idx
                )

    def process_all(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®"""
        print("=" * 80)
        print("ğŸš€ MMWHSæ•°æ®é›†é¢„å¤„ç†")
        print("=" * 80)

        self.process_split('train')
        self.process_split('test')

        self.generate_reports()

    def generate_reports(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š")
        print("="*80)

        self.generate_text_report()
        self.save_stats_json()

        print("\n" + "="*80)
        print(f"âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
        print(f"   æ•°æ®ä¿å­˜åœ¨: {self.output_root}")
        print(f"   æŠ¥å‘Šæ–‡ä»¶:")
        print(f"     - quality_report.txt")
        print(f"     - stats.json")
        print("="*80)

    def generate_text_report(self):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("MMWHSæ•°æ®é›†å¤„ç†æŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append("")

        # è®­ç»ƒé›†ç»Ÿè®¡
        report_lines.append("ã€è®­ç»ƒé›†ç»Ÿè®¡ã€‘")
        report_lines.append("")

        if len(self.stats['train']['ct']['volumes']) > 0:
            report_lines.append("CTè®­ç»ƒæ•°æ®ï¼š")
            report_lines.append(f"  Volumeæ•°é‡: {len(self.stats['train']['ct']['volumes'])}")
            report_lines.append(f"  æ€»åˆ‡ç‰‡æ•°: {sum(self.stats['train']['ct']['slices'])}")
            report_lines.append(f"  å¹³å‡æ¯ä¸ªvolume: {np.mean(self.stats['train']['ct']['slices']):.1f} åˆ‡ç‰‡")
            report_lines.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {np.mean(self.stats['train']['ct']['quality']):.3f}")
            report_lines.append("")

        if len(self.stats['train']['mr']['volumes']) > 0:
            report_lines.append("MRè®­ç»ƒæ•°æ®ï¼š")
            report_lines.append(f"  Volumeæ•°é‡: {len(self.stats['train']['mr']['volumes'])}")
            report_lines.append(f"  æ€»åˆ‡ç‰‡æ•°: {sum(self.stats['train']['mr']['slices'])}")
            report_lines.append(f"  å¹³å‡æ¯ä¸ªvolume: {np.mean(self.stats['train']['mr']['slices']):.1f} åˆ‡ç‰‡")
            report_lines.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {np.mean(self.stats['train']['mr']['quality']):.3f}")
            report_lines.append("")

        # æµ‹è¯•é›†ç»Ÿè®¡
        report_lines.append("ã€æµ‹è¯•é›†ç»Ÿè®¡ã€‘")
        report_lines.append("")

        if len(self.stats['test']['ct']['volumes']) > 0:
            report_lines.append("CTæµ‹è¯•æ•°æ®ï¼š")
            report_lines.append(f"  Volumeæ•°é‡: {len(self.stats['test']['ct']['volumes'])}")
            report_lines.append(f"  æ€»åˆ‡ç‰‡æ•°: {sum(self.stats['test']['ct']['slices'])}")
            report_lines.append(f"  å¹³å‡æ¯ä¸ªvolume: {np.mean(self.stats['test']['ct']['slices']):.1f} åˆ‡ç‰‡")
            report_lines.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {np.mean(self.stats['test']['ct']['quality']):.3f}")
            report_lines.append("")

        if len(self.stats['test']['mr']['volumes']) > 0:
            report_lines.append("MRæµ‹è¯•æ•°æ®ï¼š")
            report_lines.append(f"  Volumeæ•°é‡: {len(self.stats['test']['mr']['volumes'])}")
            report_lines.append(f"  æ€»åˆ‡ç‰‡æ•°: {sum(self.stats['test']['mr']['slices'])}")
            report_lines.append(f"  å¹³å‡æ¯ä¸ªvolume: {np.mean(self.stats['test']['mr']['slices']):.1f} åˆ‡ç‰‡")
            report_lines.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {np.mean(self.stats['test']['mr']['quality']):.3f}")
            report_lines.append("")

        # æ€»ä½“ç»Ÿè®¡
        report_lines.append("ã€æ€»ä½“ç»Ÿè®¡ã€‘")
        total_train_ct = sum(self.stats['train']['ct']['slices']) if self.stats['train']['ct']['slices'] else 0
        total_train_mr = sum(self.stats['train']['mr']['slices']) if self.stats['train']['mr']['slices'] else 0
        total_test_ct = sum(self.stats['test']['ct']['slices']) if self.stats['test']['ct']['slices'] else 0
        total_test_mr = sum(self.stats['test']['mr']['slices']) if self.stats['test']['mr']['slices'] else 0

        report_lines.append(f"  è®­ç»ƒé›†æ€»åˆ‡ç‰‡: {total_train_ct + total_train_mr}")
        report_lines.append(f"    - CT: {total_train_ct}")
        report_lines.append(f"    - MR: {total_train_mr}")
        report_lines.append(f"  æµ‹è¯•é›†æ€»åˆ‡ç‰‡: {total_test_ct + total_test_mr}")
        report_lines.append(f"    - CT: {total_test_ct}")
        report_lines.append(f"    - MR: {total_test_mr}")
        report_lines.append(f"  æ€»è®¡åˆ‡ç‰‡æ•°: {total_train_ct + total_train_mr + total_test_ct + total_test_mr}")
        report_lines.append("")

        report_lines.append("=" * 80)

        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report_lines)
        with open(self.output_root / 'quality_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(report_text)

    def save_stats_json(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSONï¼ˆä¿®å¤ç‰ˆï¼‰"""

        def convert_to_serializable(obj):
            """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_to_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [convert_to_serializable(item) for item in obj]
            else:
                return obj

        # è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
        stats_serializable = {}

        for split in ['train', 'test']:
            stats_serializable[split] = {}

            for modality in ['ct', 'mr']:
                stats_serializable[split][modality] = {
                    'volumes': [int(v) for v in self.stats[split][modality]['volumes']],
                    'slices': [int(s) for s in self.stats[split][modality]['slices']],
                    'quality': [float(q) for q in self.stats[split][modality]['quality']],
                    'spacing': [[float(x) for x in s] for s in self.stats[split][modality]['spacing']],
                    'dimensions': [[int(x) for x in d] for d in self.stats[split][modality]['dimensions']]
                }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.output_root / 'stats.json', 'w', encoding='utf-8') as f:
            json.dump(stats_serializable, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {self.output_root / 'stats.json'}")


# ä¸»å‡½æ•°
if __name__ == '__main__':
    data_root = r'D:\AåŸºäºUNetå®ç°å¤šæ¨¡æ€è·¨åŸŸè‡ªé€‚åº”\unet\Pytorch-UNet-master\data\mmwhs'
    output_root = r'D:\AåŸºäºUNetå®ç°å¤šæ¨¡æ€è·¨åŸŸè‡ªé€‚åº”\unet\Pytorch-UNet-master\data\mmwhs_processed'

    processor = MMWHSPreprocessor(data_root, output_root)
    processor.process_all()

    print("\nâœ… é¢„å¤„ç†å®Œæˆï¼")